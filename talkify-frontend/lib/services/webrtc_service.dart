import 'dart:async';
import 'package:flutter/material.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart' as webrtc;
import 'package:permission_handler/permission_handler.dart';
import 'package:get/get.dart';
import 'socket_service.dart';

/// WebRTC Service for Audio/Video Calling
class WebRTCService {
  static webrtc.RTCPeerConnection? _peerConnection;
  static webrtc.MediaStream? _localStream;
  static webrtc.MediaStream? _remoteStream;
  static webrtc.RTCVideoRenderer? _localRenderer;
  static webrtc.RTCVideoRenderer? _remoteRenderer;
  static String? _currentRoomId;
  static String? _remoteUserId;
  
  // ICE candidate queue for timing issues
  static final List<webrtc.RTCIceCandidate> _iceCandidateQueue = [];
  static bool _isPeerConnectionReady = false;
  
  // Callbacks
  static Function(webrtc.MediaStream)? onLocalStream;
  static Function(webrtc.MediaStream)? onRemoteStream;
  static Function()? onCallConnected;
  static Function()? onCallDisconnected;
  static Function(String)? onCallError;

  /// Initialize WebRTC service
  static Future<bool> initialize() async {
    try {
      print('ğŸ¤ Initializing WebRTC service...');
      
      // Initialize renderers
      _localRenderer = webrtc.RTCVideoRenderer();
      _remoteRenderer = webrtc.RTCVideoRenderer();
      
      await _localRenderer!.initialize();
      await _remoteRenderer!.initialize();
      
      print('âœ… WebRTC service initialized successfully');
      return true;
    } catch (e) {
      print('âŒ Failed to initialize WebRTC service: $e');
      return false;
    }
  }

  /// Check and request microphone permission
  static Future<bool> requestMicrophonePermission() async {
    try {
      print('========================================');
      print('ğŸ¤ === REQUESTING MICROPHONE PERMISSION ===');
      print('========================================');
      
      // Check current permission status
      PermissionStatus status = await Permission.microphone.status;
      print('ğŸ¤ Current microphone permission status: $status');
      
      if (status.isGranted) {
        print('âœ… âœ… âœ… Microphone permission ALREADY GRANTED');
        print('========================================');
        return true;
      }
      
      if (status.isDenied) {
        print('âš ï¸ Microphone permission is DENIED - requesting now...');
        
        // Request permission
        status = await Permission.microphone.request();
        print('ğŸ¤ After request, permission status: $status');
        
        if (status.isGranted) {
          print('âœ… âœ… âœ… Microphone permission GRANTED by user');
          print('========================================');
          return true;
        } else if (status.isDenied) {
          print('âŒ âŒ âŒ User DENIED microphone permission');
          print('========================================');
          _showPermissionDeniedDialog();
          return false;
        } else if (status.isPermanentlyDenied) {
          print('âŒ âŒ âŒ User PERMANENTLY DENIED microphone permission');
          print('========================================');
          _showPermissionPermanentlyDeniedDialog();
          return false;
        }
      }
      
      if (status.isPermanentlyDenied) {
        print('âŒ âŒ âŒ Microphone permission is PERMANENTLY DENIED');
        print('ğŸ”§ User must enable in device Settings â†’ App Permissions â†’ Microphone');
        print('========================================');
        _showPermissionPermanentlyDeniedDialog();
        return false;
      }
      
      if (status.isRestricted) {
        print('âŒ Microphone permission is RESTRICTED (parental controls?)');
        print('========================================');
        return false;
      }
      
      print('âš ï¸ Unknown permission status: $status');
      print('========================================');
      return false;
    } catch (e) {
      print('âŒ âŒ âŒ ERROR requesting microphone permission: $e');
      print('Stack trace: ${StackTrace.current}');
      print('========================================');
      return false;
    }
  }

  /// Check and request camera permission
  static Future<bool> requestCameraPermission() async {
    try {
      print('========================================');
      print('ğŸ“¹ === REQUESTING CAMERA PERMISSION ===');
      print('========================================');
      
      PermissionStatus status = await Permission.camera.status;
      print('ğŸ“¹ Current camera permission status: $status');
      
      if (status.isGranted) {
        print('âœ… âœ… âœ… Camera permission ALREADY GRANTED');
        print('========================================');
        return true;
      }
      
      if (status.isDenied) {
        print('âš ï¸ Camera permission is DENIED - requesting now...');
        status = await Permission.camera.request();
        print('ğŸ“¹ After request, permission status: $status');
        
        if (status.isGranted) {
          print('âœ… âœ… âœ… Camera permission GRANTED by user');
          print('========================================');
          return true;
        } else if (status.isDenied) {
          print('âŒ âŒ âŒ User DENIED camera permission');
          print('========================================');
          _showPermissionDeniedDialog();
          return false;
        } else if (status.isPermanentlyDenied) {
          print('âŒ âŒ âŒ User PERMANENTLY DENIED camera permission');
          print('========================================');
          _showPermissionPermanentlyDeniedDialog();
          return false;
        }
      }
      
      if (status.isPermanentlyDenied) {
        print('âŒ âŒ âŒ Camera permission is PERMANENTLY DENIED');
        print('ğŸ”§ User must enable in device Settings â†’ App Permissions â†’ Camera');
        print('========================================');
        _showPermissionPermanentlyDeniedDialog();
        return false;
      }
      
      if (status.isRestricted) {
        print('âŒ Camera permission is RESTRICTED (parental controls?)');
        print('========================================');
        return false;
      }
      
      print('âš ï¸ Unknown permission status: $status');
      print('========================================');
      return false;
    } catch (e) {
      print('âŒ âŒ âŒ ERROR requesting camera permission: $e');
      print('Stack trace: ${StackTrace.current}');
      print('========================================');
      return false;
    }
  }

  /// Start local media stream (audio/video based on call type)
  static Future<webrtc.MediaStream?> startLocalStream({String callType = 'audio'}) async {
    try {
      print('========================================');
      print('ğŸ¤ === STARTING LOCAL MEDIA STREAM ===');
      print('ğŸ¤ Call type: $callType');
      print('========================================');
      
      // Request microphone permission first - CRITICAL: Must succeed before getUserMedia
      bool hasPermission = await requestMicrophonePermission();
      if (!hasPermission) {
        print('========================================');
        print('âŒ âŒ âŒ CANNOT START CALL: Microphone permission NOT granted');
        print('========================================');
        onCallError?.call('Microphone permission denied. Please enable microphone access in Settings.');
        return null;
      }
      
      print('========================================');
      print('âœ… âœ… âœ… Microphone permission confirmed GRANTED');
      print('========================================');
      
      // Request camera permission for video calls - CRITICAL: Must succeed before getUserMedia
      bool hasVideoPermission = true;
      if (callType == 'video') {
        hasVideoPermission = await requestCameraPermission();
        if (!hasVideoPermission) {
          print('========================================');
          print('âŒ âŒ âŒ CANNOT START VIDEO CALL: Camera permission NOT granted');
          print('========================================');
          onCallError?.call('Camera permission denied. Please enable camera access in Settings.');
          return null;
        }
        
        print('========================================');
        print('âœ… âœ… âœ… Camera permission confirmed GRANTED');
        print('========================================');
      }
      
      // Get user media based on call type
      Map<String, dynamic> mediaConstraints = {
        'audio': {
          'echoCancellation': true,
          'noiseSuppression': true,
          'autoGainControl': true,
          'sampleRate': 44100,
        },
        'video': callType == 'video' ? {
          'width': {'ideal': 1280},
          'height': {'ideal': 720},
          'frameRate': {'ideal': 30},
          'facingMode': 'user', // Front camera
        } : false,
      };
      
      print('ğŸ¤ Media constraints:');
      print('ğŸ¤ - Audio: ENABLED');
      print('ğŸ“¹ - Video: ${callType == 'video' ? "ENABLED" : "DISABLED"}');
      if (callType == 'video') {
        print('ğŸ“¹ - Video constraints: 1280x720@30fps, front camera');
      }
      
      print('========================================');
      print('ğŸ¤ === CALLING getUserMedia() ===');
      print('ğŸ¤ This will access the microphone and camera');
      print('ğŸ¤ Permissions have been confirmed as granted');
      print('========================================');
      
      try {
        _localStream = await webrtc.navigator.mediaDevices.getUserMedia(mediaConstraints);
      } catch (e) {
        print('========================================');
        print('âŒ âŒ âŒ getUserMedia() FAILED');
        print('âŒ Error: $e');
        print('âŒ This usually means:');
        print('   1. Permissions were revoked after granting');
        print('   2. Device does not have camera/microphone');
        print('   3. Camera/microphone is in use by another app');
        print('========================================');
        onCallError?.call('Failed to access camera/microphone: $e');
        return null;
      }
      
      if (_localStream != null) {
        print('========================================');
        print('âœ… âœ… âœ… LOCAL MEDIA STREAM STARTED SUCCESSFULLY');
        print('========================================');
        print('ğŸ”Š Stream ID: ${_localStream!.id}');
        print('ğŸ”Š Total tracks: ${_localStream!.getTracks().length}');
        
        int audioTracks = 0;
        int videoTracks = 0;
        
        for (var track in _localStream!.getTracks()) {
          if (track.kind == 'audio') audioTracks++;
          if (track.kind == 'video') videoTracks++;
          
          print('========================================');
          print('ğŸ”Š Track ${track.kind?.toUpperCase()}:');
          print('ğŸ”Š - ID: ${track.id}');
          print('ğŸ”Š - Label: ${track.label}');
          print('ğŸ”Š - Enabled: ${track.enabled}');
          print('ğŸ”Š - Muted: ${track.muted}');
          print('ğŸ”Š - Ready State: ${track.label}');
          print('========================================');
          
          // Set up track event listeners
          track.onEnded = () {
            print('âš ï¸ âš ï¸ âš ï¸ Local ${track.kind} track ENDED!');
            onCallError?.call('Local ${track.kind} track ended');
          };
          
          track.onMute = () {
            print('âš ï¸ Local ${track.kind} track MUTED');
          };
          
          track.onUnMute = () {
            print('âœ… Local ${track.kind} track UNMUTED');
          };
        }
        
        print('ğŸ“Š Summary:');
        print('ğŸ“Š - Audio tracks: $audioTracks');
        print('ğŸ“Š - Video tracks: $videoTracks');
        print('ğŸ“Š - Expected: Audio=1, Video=${callType == 'video' ? '1' : '0'}');
        
        if (audioTracks == 0) {
          print('âŒ WARNING: No audio tracks found!');
        }
        
        if (callType == 'video' && videoTracks == 0) {
          print('âŒ WARNING: Video call requested but no video tracks found!');
        }
        
        print('========================================');
        
        // Set local stream to local renderer
        if (_localRenderer != null) {
          print('ğŸ“¹ === SETTING LOCAL STREAM TO LOCAL RENDERER ===');
          _localRenderer!.srcObject = _localStream;
          print('ğŸ“¹ Local renderer srcObject: ${_localRenderer!.srcObject != null ? "SUCCESS" : "FAILED"}');
          
          if (_localRenderer!.srcObject != null) {
            print('âœ… âœ… âœ… Local video preview is ready!');
          } else {
            print('âŒ Failed to set local stream to renderer');
          }
        } else {
          print('âš ï¸ Local renderer is null - local preview not available');
        }
        
        // Monitor stream health
        _monitorStreamHealth();
        
        print('ğŸ“ Calling onLocalStream callback...');
        onLocalStream?.call(_localStream!);
        print('========================================');
        return _localStream;
      } else {
        print('âŒ âŒ âŒ Failed to start local media stream');
        onCallError?.call('Failed to start local media stream');
        return null;
      }
    } catch (e) {
      print('âŒ Error starting local audio stream: $e');
      onCallError?.call('Failed to start audio: $e');
      return null;
    }
  }

  /// Create peer connection
  static Future<webrtc.RTCPeerConnection?> createPeerConnection({String callType = 'audio'}) async {
  try {
    print('ğŸ”— Creating peer connection for $callType call...');

    // âœ… Correct configuration as Map
    final Map<String, dynamic> configuration = {
      'iceServers': [
        {'urls': 'stun:stun.l.google.com:19302'},
        {'urls': 'stun:stun1.l.google.com:19302'},
      ],
      'iceCandidatePoolSize': 10,
    };

    final Map<String, dynamic> constraints = {
      'mandatory': {
        'OfferToReceiveAudio': true,
        'OfferToReceiveVideo': callType == 'video',
      },
      'optional': [],
    };

    _peerConnection = await webrtc.createPeerConnection(configuration, constraints);
    
    if (_peerConnection == null) {
      print('âŒ Failed to create peer connection - returned null');
      return null;
    }
    
    print('âœ… Peer connection created successfully');
    
    // Mark peer connection as ready and process queued ICE candidates
    _isPeerConnectionReady = true;
    _processQueuedIceCandidates();

    // Set up event handlers
    _peerConnection!.onIceCandidate = (webrtc.RTCIceCandidate candidate) {
      print('ğŸ§Š === ICE CANDIDATE GENERATED ===');
      print('ğŸ§Š Candidate: ${candidate.candidate}');
      print('ğŸ§Š SDP MID: ${candidate.sdpMid}');
      print('ğŸ§Š SDP MLine Index: ${candidate.sdpMLineIndex}');
      print('ğŸ§Š Sending to remote peer...');
      _sendIceCandidate(candidate);
      print('ğŸ§Š ICE candidate sent');
    };

    _peerConnection!.onTrack = (webrtc.RTCTrackEvent event) async {
      print('========================================');
      print('ğŸ“º === REMOTE TRACK EVENT RECEIVED ===');
      print('========================================');
      print('ğŸ“º Track kind: ${event.track.kind}');
      print('ğŸ“º Track ID: ${event.track.id}');
      print('ğŸ“º Track label: ${event.track.label}');
      print('ğŸ“º Track enabled: ${event.track.enabled}');
      print('ğŸ“º Track muted: ${event.track.muted}');
      print('ğŸ“º Track readyState: ${event.track.label}');
      print('ğŸ“º Event streams count: ${event.streams.length}');
      
      if (event.streams.isNotEmpty) {
        _remoteStream = event.streams[0];
        print('ğŸ”Š === REMOTE STREAM RECEIVED ===');
        print('ğŸ”Š Stream ID: ${_remoteStream!.id}');
        print('ğŸ”Š Total tracks in remote stream: ${_remoteStream!.getTracks().length}');
        
        // Log all tracks in detail
        int trackIndex = 0;
        for (var track in _remoteStream!.getTracks()) {
          trackIndex++;
          print('ğŸ”Š --- Track #$trackIndex ---');
          print('ğŸ”Š Kind: ${track.kind}');
          print('ğŸ”Š ID: ${track.id}');
          print('ğŸ”Š Label: ${track.label}');
          print('ğŸ”Š Enabled: ${track.enabled}');
          print('ğŸ”Š Muted: ${track.muted}');
          
          // Handle audio tracks
          if (track.kind == 'audio') {
            track.enabled = true;
            print('ğŸ”Š âœ… Remote AUDIO track force-enabled: ${track.enabled}');
            
            // Set up track event listeners
            track.onEnded = () {
              print('âš ï¸ âš ï¸ âš ï¸ Remote audio track ENDED!');
            };
            track.onMute = () {
              print('âš ï¸ Remote audio track MUTED');
            };
            track.onUnMute = () {
              print('âœ… Remote audio track UNMUTED');
            };
          }
          
          // Handle video tracks
          if (track.kind == 'video') {
            track.enabled = true;
            print('ğŸ“¹ âœ… Remote VIDEO track force-enabled: ${track.enabled}');
            
            // Set up track event listeners
            track.onEnded = () {
              print('âš ï¸ âš ï¸ âš ï¸ Remote video track ENDED!');
            };
            track.onMute = () {
              print('âš ï¸ Remote video track MUTED');
            };
            track.onUnMute = () {
              print('âœ… Remote video track UNMUTED');
            };
          }
        }
        
        // Set the remote stream to the renderer
        print('ğŸ”Š === SETTING REMOTE STREAM TO RENDERER ===');
        if (_remoteRenderer != null) {
          print('ğŸ”Š Remote renderer exists, setting srcObject...');
          _remoteRenderer!.srcObject = _remoteStream;
          print('ğŸ”Š srcObject assignment result: ${_remoteRenderer!.srcObject != null ? "SUCCESS" : "FAILED"}');
          
          if (_remoteRenderer!.srcObject != null) {
            print('âœ… âœ… âœ… Remote stream successfully set to renderer!');
            print('ğŸ”Š Renderer stream ID: ${_remoteRenderer!.srcObject?.id ?? "N/A"}');
          } else {
            print('âŒ âŒ âŒ CRITICAL: Failed to set srcObject on first attempt!');
            print('ğŸ”„ Retrying in 100ms...');
            Future.delayed(const Duration(milliseconds: 100), () {
              _remoteRenderer!.srcObject = _remoteStream;
              print('ğŸ”„ Retry result: ${_remoteRenderer!.srcObject != null ? "SUCCESS" : "FAILED"}');
            });
          }
        } else {
          print('âŒ CRITICAL: Remote renderer is null!');
          print('ğŸ”„ Initializing new renderer...');
          _remoteRenderer = webrtc.RTCVideoRenderer();
          await _remoteRenderer!.initialize();
          _remoteRenderer!.srcObject = _remoteStream;
          print('ğŸ”„ New renderer initialized and stream set: ${_remoteRenderer!.srcObject != null ? "SUCCESS" : "FAILED"}');
        }
        
        print('ğŸ“ Calling onRemoteStream callback...');
        onRemoteStream?.call(_remoteStream!);
        print('========================================');
      } else {
        print('âŒ âŒ âŒ CRITICAL: No streams in track event!');
        print('========================================');
      }
    };

    _peerConnection!.onConnectionState = (webrtc.RTCPeerConnectionState state) {
      print('ğŸ”— Connection state: $state');
      switch (state) {
        case webrtc.RTCPeerConnectionState.RTCPeerConnectionStateConnected:
          print('âœ… WebRTC connection established');
          onCallConnected?.call();
          break;
        case webrtc.RTCPeerConnectionState.RTCPeerConnectionStateDisconnected:
          print('âš ï¸ WebRTC connection disconnected - attempting to reconnect');
          // Don't immediately call onCallDisconnected for disconnected state
          // as it might be temporary
          break;
        case webrtc.RTCPeerConnectionState.RTCPeerConnectionStateFailed:
          print('âŒ WebRTC connection failed');
          onCallError?.call('Connection failed');
          onCallDisconnected?.call();
          break;
        case webrtc.RTCPeerConnectionState.RTCPeerConnectionStateClosed:
          print('ğŸ”’ WebRTC connection closed');
          onCallDisconnected?.call();
          break;
        case webrtc.RTCPeerConnectionState.RTCPeerConnectionStateConnecting:
          print('ğŸ”„ WebRTC connection connecting...');
          break;
        case webrtc.RTCPeerConnectionState.RTCPeerConnectionStateNew:
          print('ğŸ†• WebRTC connection new');
          break;
      }
    };

    print('âœ… Peer connection created');
    return _peerConnection;
  } catch (e) {
    print('âŒ Error creating peer connection: $e');
    onCallError?.call('Failed to create connection: $e');
    return null;
  }
}


  /// Start outgoing call
  static Future<bool> startOutgoingCall(String roomId, String userId, String remoteUserId, {String callType = 'audio'}) async {
    try {
      print('ğŸ“ Starting outgoing call to $remoteUserId in room $roomId');
      
      _currentRoomId = roomId;
      _remoteUserId = remoteUserId;
      
      // Reset ICE candidate queue for new call
      _iceCandidateQueue.clear();
      _isPeerConnectionReady = false;
      
      // Initialize renderers if not already done
      if (_localRenderer == null) {
        _localRenderer = webrtc.RTCVideoRenderer();
        await _localRenderer!.initialize();
        print('âœ… Local renderer initialized');
      }
      
      if (_remoteRenderer == null) {
        _remoteRenderer = webrtc.RTCVideoRenderer();
        await _remoteRenderer!.initialize();
        print('âœ… Remote renderer initialized');
      }
      
      // Start local stream
      webrtc.MediaStream? localStream = await startLocalStream(callType: callType);
      if (localStream == null) {
        return false;
      }
      
      // Create peer connection
      webrtc.RTCPeerConnection? peerConnection = await createPeerConnection(callType: callType);
      if (peerConnection == null) {
        return false;
      }
      
      // Store the peer connection globally
      _peerConnection = peerConnection;
      
      // Add local stream tracks to peer connection
      print('ğŸ”Š === ADDING LOCAL TRACKS TO PEER CONNECTION ===');
      try {
        final tracks = localStream.getTracks();
        print('ğŸ”Š Total tracks to add: ${tracks.length}');
        for (var track in tracks) {
          print('ğŸ”Š Adding track: ${track.kind}, enabled: ${track.enabled}, readyState: ${track.label}');
          await peerConnection.addTrack(track, localStream);
          print('âœ… Successfully added ${track.kind} track');
        }
      } catch (e) {
        print('âŒ Error adding tracks: $e');
        throw Exception('Failed to add tracks to peer connection: $e');
      }
      
      // Create offer
      print('ğŸ“¡ === CREATING OFFER ===');
      webrtc.RTCSessionDescription offer = await peerConnection.createOffer();
      print('ğŸ“¡ Offer created - type: ${offer.type}');
      print('ğŸ“¡ Offer SDP length: ${offer.sdp?.length ?? 0}');
      print('ğŸ“¡ Setting local description...');
      await peerConnection.setLocalDescription(offer);
      print('âœ… Local description set');
      
      // Send offer via Socket.IO
      print('ğŸ“¡ Sending offer via Socket.IO to $remoteUserId in room $roomId');
      _sendOffer(offer, roomId, remoteUserId);
      
      print('âœ… Outgoing call started');
      return true;
    } catch (e) {
      print('âŒ Error starting outgoing call: $e');
      onCallError?.call('Failed to start call: $e');
      return false;
    }
  }

  /// Accept incoming call
  static Future<bool> acceptIncomingCall(String roomId, String userId, String remoteUserId, {String callType = 'audio'}) async {
    try {
      print('ğŸ“ Accepting incoming call from $remoteUserId in room $roomId');
      
      _currentRoomId = roomId;
      _remoteUserId = remoteUserId;
      
      // Reset ICE candidate queue for new call
      _iceCandidateQueue.clear();
      _isPeerConnectionReady = false;
      
      // Initialize renderers if not already done
      if (_localRenderer == null) {
        _localRenderer = webrtc.RTCVideoRenderer();
        await _localRenderer!.initialize();
        print('âœ… Local renderer initialized');
      }
      
      if (_remoteRenderer == null) {
        _remoteRenderer = webrtc.RTCVideoRenderer();
        await _remoteRenderer!.initialize();
        print('âœ… Remote renderer initialized');
      }
      
      // Start local stream
      webrtc.MediaStream? localStream = await startLocalStream(callType: callType);
      if (localStream == null) {
        return false;
      }
      
      // Create peer connection
      webrtc.RTCPeerConnection? peerConnection = await createPeerConnection(callType: callType);
      if (peerConnection == null) {
        return false;
      }
      
      // Store the peer connection globally
      _peerConnection = peerConnection;
      
      // NOTE: DO NOT add tracks here!
      // Tracks will be added in handleOffer() AFTER setting remote description
      // This is the correct WebRTC order: setRemoteDescription -> addTrack -> createAnswer
      print('ğŸ”Š Local stream ready with ${localStream.getTracks().length} tracks');
      print('ğŸ“ Peer connection ready - tracks will be added when processing offer');
      
      print('âœ… Ready to accept incoming call');
      return true;
    } catch (e) {
      print('âŒ Error accepting incoming call: $e');
      onCallError?.call('Failed to accept call: $e');
      return false;
    }
  }

  /// Handle incoming offer
  static Future<void> handleOffer(webrtc.RTCSessionDescription offer, String roomId, String fromUserId, {String callType = 'audio'}) async {
    try {
      print('ğŸ“¨ Handling incoming offer from $fromUserId');
      print('ğŸ“¨ Offer type: ${offer.type}');
      print('ğŸ“¨ Offer SDP length: ${offer.sdp?.length ?? 0}');
      
      // Store room and user info
      _currentRoomId = roomId;
      _remoteUserId = fromUserId;
      
      // Reset ICE candidate queue for new call
      _iceCandidateQueue.clear();
      _isPeerConnectionReady = false;
      
      if (_peerConnection == null) {
        print('âŒ No peer connection available, creating one...');
        await createPeerConnection(callType: callType);
        if (_peerConnection == null) {
          print('âŒ Failed to create peer connection');
          onCallError?.call('Failed to create peer connection');
          return;
        }
        print('âœ… Peer connection created in handleOffer');
      } else {
        print('âœ… Peer connection already exists');
      }
      
      print('ğŸ“¨ Setting remote description (offer)...');
      await _peerConnection!.setRemoteDescription(offer);
      print('âœ… Remote description (offer) set successfully');
      print('ğŸ“¨ Peer connection signaling state: ${_peerConnection!.signalingState}');
      
      // CRITICAL: Local stream should already be started by acceptIncomingCall
      if (_localStream == null) {
        print('âš ï¸âš ï¸âš ï¸ WARNING: Local stream is NULL! Starting it now (this should not happen)...');
        await startLocalStream(callType: callType);
      } else {
        print('âœ… Local stream already available with ${_localStream!.getTracks().length} tracks');
      }
      
      // CRITICAL: Add local stream tracks to peer connection AFTER setting remote description
      // This is the correct WebRTC order for answerer: setRemoteDescription -> addTrack -> createAnswer
      if (_localStream != null) {
        print('========================================');
        print('ğŸ”Š === ADDING LOCAL TRACKS TO PEER CONNECTION (ANSWERER) ===');
        print('ğŸ”Š This is User B adding their video/audio to send to User A');
        print('========================================');
        try {
          final tracks = _localStream!.getTracks();
          print('ğŸ”Š Total local tracks available: ${tracks.length}');
          
          int audioCount = 0;
          int videoCount = 0;
          
          for (var track in tracks) {
            if (track.kind == 'audio') audioCount++;
            if (track.kind == 'video') videoCount++;
            
            print('========================================');
            print('ğŸ”Š Adding ${track.kind?.toUpperCase()} track to peer connection');
            print('ğŸ”Š - Track ID: ${track.id}');
            print('ğŸ”Š - Track label: ${track.label}');
            print('ğŸ”Š - Track enabled: ${track.enabled}');
            print('ğŸ”Š - Track muted: ${track.muted}');
            print('========================================');
            
            await _peerConnection!.addTrack(track, _localStream!);
            print('âœ… âœ… âœ… Successfully added local ${track.kind} track to peer connection');
          }
          
          print('========================================');
          print('ğŸ“Š TRACK SUMMARY FOR ANSWER:');
          print('ğŸ“Š - Audio tracks added: $audioCount');
          print('ğŸ“Š - Video tracks added: $videoCount');
          print('ğŸ“Š - Expected for $callType call: Audio=1, Video=${callType == 'video' ? '1' : '0'}');
          print('========================================');
          
          if (audioCount == 0) {
            print('âŒâŒâŒ CRITICAL: No audio tracks added! User A will not hear User B!');
          }
          
          if (callType == 'video' && videoCount == 0) {
            print('âŒâŒâŒ CRITICAL: No video tracks added! User A will not see User B!');
          }
          
        } catch (e) {
          print('âŒâŒâŒ CRITICAL ERROR adding local tracks: $e');
          print('âŒ This means User A will NOT receive User B\'s video/audio!');
        }
      } else {
        print('âŒâŒâŒ CRITICAL: No local stream available when creating answer!');
        print('âŒ User A will NOT receive User B\'s video/audio!');
      }
      
      // Create answer
      print('========================================');
      print('ğŸ“¨ === CREATING ANSWER ===');
      print('ğŸ“¨ Creating answer with local tracks included');
      print('========================================');
      webrtc.RTCSessionDescription answer = await _peerConnection!.createAnswer();
      print('âœ… Answer created successfully');
      print('ğŸ“¨ Answer type: ${answer.type}');
      print('ğŸ“¨ Answer SDP length: ${answer.sdp?.length ?? 0}');
      
      // Verify SDP includes media
      if (answer.sdp != null) {
        final hasAudio = answer.sdp!.contains('m=audio');
        final hasVideo = answer.sdp!.contains('m=video');
        print('========================================');
        print('ğŸ“¨ SDP ANALYSIS:');
        print('ğŸ“¨ - Contains audio media line: $hasAudio');
        print('ğŸ“¨ - Contains video media line: $hasVideo');
        print('ğŸ“¨ - Expected for $callType call: Audio=$hasAudio, Video=${callType == 'video' ? hasVideo : 'N/A'}');
        print('========================================');
        
        if (!hasAudio) {
          print('âŒâŒâŒ CRITICAL: Answer SDP does NOT contain audio! User A will not hear User B!');
        }
        
        if (callType == 'video' && !hasVideo) {
          print('âŒâŒâŒ CRITICAL: Answer SDP does NOT contain video! User A will not see User B!');
        }
      }
      
      print('ğŸ“¨ Setting local description (answer)...');
      await _peerConnection!.setLocalDescription(answer);
      print('âœ… Local description (answer) set');
      
      // Send answer via Socket.IO
      print('ğŸ“¨ Sending answer via Socket.IO to $fromUserId in room $roomId');
      _sendAnswer(answer, roomId, fromUserId);
      
      print('âœ… Offer handled and answer sent');
    } catch (e) {
      print('âŒ Error handling offer: $e');
      onCallError?.call('Failed to handle offer: $e');
    }
  }

  /// Handle incoming answer
  static Future<void> handleAnswer(webrtc.RTCSessionDescription answer) async {
    try {
      print('========================================');
      print('ğŸ“¨ === USER A: HANDLING INCOMING ANSWER FROM USER B ===');
      print('ğŸ“¨ This is User A receiving User B\'s answer');
      print('========================================');
      print('ğŸ“¨ Answer type: ${answer.type}');
      print('ğŸ“¨ Answer SDP length: ${answer.sdp?.length ?? 0}');
      
      // Verify SDP includes media
      if (answer.sdp != null) {
        final hasAudio = answer.sdp!.contains('m=audio');
        final hasVideo = answer.sdp!.contains('m=video');
        print('========================================');
        print('ğŸ“¨ ANSWER SDP ANALYSIS:');
        print('ğŸ“¨ - Contains audio media line: $hasAudio');
        print('ğŸ“¨ - Contains video media line: $hasVideo');
        print('========================================');
        
        if (!hasAudio) {
          print('âŒâŒâŒ CRITICAL: Answer SDP does NOT contain audio! User A will NOT hear User B!');
        } else {
          print('âœ… Answer SDP contains audio - User A should hear User B');
        }
        
        if (!hasVideo) {
          print('âš ï¸ Answer SDP does NOT contain video (might be audio-only call or issue)');
        } else {
          print('âœ… Answer SDP contains video - User A should see User B');
        }
      }
      
      if (_peerConnection == null) {
        print('âŒâŒâŒ CRITICAL: No peer connection available in handleAnswer');
        return;
      }
      print('âœ… Peer connection available in handleAnswer');
      print('ğŸ“¨ Current signaling state: ${_peerConnection!.signalingState}');
      
      print('========================================');
      print('ğŸ“¨ Setting remote description (answer) on User A\'s peer connection...');
      print('ğŸ“¨ This will complete the signaling handshake');
      print('========================================');
      await _peerConnection!.setRemoteDescription(answer);
      print('âœ… âœ… âœ… Remote description (answer) set successfully');
      print('ğŸ“¨ New signaling state: ${_peerConnection!.signalingState}');
      
      // Mark peer connection as ready for ICE candidates
      _isPeerConnectionReady = true;
      print('âœ… Peer connection marked as ready for ICE candidates');
      
      // Process queued ICE candidates
      if (_iceCandidateQueue.isNotEmpty) {
        print('========================================');
        print('ğŸ§Š Processing ${_iceCandidateQueue.length} queued ICE candidates');
        print('========================================');
        for (var i = 0; i < _iceCandidateQueue.length; i++) {
          try {
            print('ğŸ§Š Adding queued ICE candidate ${i + 1}/${_iceCandidateQueue.length}');
            await _peerConnection!.addCandidate(_iceCandidateQueue[i]);
            print('âœ… Added queued ICE candidate ${i + 1}');
          } catch (e) {
            print('âŒ Error adding queued ICE candidate ${i + 1}: $e');
          }
        }
        _iceCandidateQueue.clear();
        print('âœ… All queued ICE candidates processed and queue cleared');
      } else {
        print('â„¹ï¸ No queued ICE candidates to process');
      }
      
      print('========================================');
      print('âœ… âœ… âœ… ANSWER HANDLED SUCCESSFULLY');
      print('ğŸ“¨ User A should now start receiving User B\'s tracks via ontrack events');
      print('ğŸ“¨ Watch for "=== REMOTE TRACK RECEIVED ===" logs next');
      print('========================================');
    } catch (e) {
      print('âŒâŒâŒ CRITICAL ERROR handling answer: $e');
      onCallError?.call('Failed to handle answer: $e');
    }
  }

  /// Handle incoming ICE candidate
  static Future<void> handleIceCandidate(webrtc.RTCIceCandidate candidate) async {
    try {
      print('ğŸ§Š Handling incoming ICE candidate');
      print('ğŸ§Š Candidate: ${candidate.candidate}');
      print('ğŸ§Š SDP MID: ${candidate.sdpMid}');
      print('ğŸ§Š SDP MLine Index: ${candidate.sdpMLineIndex}');
      
      if (_peerConnection == null || !_isPeerConnectionReady) {
        print('âš ï¸ Peer connection not ready, queuing ICE candidate');
        _iceCandidateQueue.add(candidate);
        print('ğŸ“‹ ICE candidate queued (${_iceCandidateQueue.length} total)');
        return;
      }
      
      print('âœ… Peer connection ready, adding ICE candidate immediately');
      await _peerConnection!.addCandidate(candidate);
      print('âœ… ICE candidate added');
    } catch (e) {
      print('âŒ Error handling ICE candidate: $e');
      // If adding fails, queue it for later
      _iceCandidateQueue.add(candidate);
      print('ğŸ“‹ ICE candidate queued due to error (${_iceCandidateQueue.length} total)');
    }
  }

  /// End call
  static Future<void> endCall() async {
    try {
      print('ğŸ“ Ending call...');
      
      // Clear ICE candidate queue
      _iceCandidateQueue.clear();
      _isPeerConnectionReady = false;
      
      // Close peer connection
      if (_peerConnection != null) {
        await _peerConnection!.close();
        _peerConnection = null;
      }
      
      // Stop local stream
      if (_localStream != null) {
        await _localStream!.dispose();
        _localStream = null;
      }
      
      // Clear remote stream
      if (_remoteStream != null) {
        _remoteStream = null;
      }
      
      // Clear renderers
      if (_localRenderer != null) {
        await _localRenderer!.dispose();
        _localRenderer = null;
      }
      
      if (_remoteRenderer != null) {
        await _remoteRenderer!.dispose();
        _remoteRenderer = null;
      }
      
      // Reset state
      _currentRoomId = null;
      _remoteUserId = null;
      
      onCallDisconnected?.call();
      print('âœ… Call ended');
    } catch (e) {
      print('âŒ Error ending call: $e');
    }
  }

  /// Get local renderer
  static webrtc.RTCVideoRenderer? get localRenderer => _localRenderer;
  
  /// Get remote renderer
  static webrtc.RTCVideoRenderer? get remoteRenderer => _remoteRenderer;
  
  /// Process queued ICE candidates when peer connection is ready
  static Future<void> _processQueuedIceCandidates() async {
    if (!_isPeerConnectionReady || _peerConnection == null) {
      print('âš ï¸ Cannot process ICE candidates - peer connection not ready');
      return;
    }
    
    print('ğŸ”„ Processing ${_iceCandidateQueue.length} queued ICE candidates...');
    
    for (var candidate in _iceCandidateQueue) {
      try {
        await _peerConnection!.addCandidate(candidate);
        print('âœ… Queued ICE candidate added: ${candidate.candidate}');
      } catch (e) {
        print('âŒ Failed to add queued ICE candidate: $e');
      }
    }
    
    _iceCandidateQueue.clear();
    print('âœ… All queued ICE candidates processed');
  }
  
  /// Manually process queued ICE candidates (for debugging)
  static Future<void> processQueuedIceCandidates() async {
    await _processQueuedIceCandidates();
  }
  
  /// Get current peer connection status for debugging
  static Map<String, dynamic> getConnectionStatus() {
    return {
      'peerConnection': _peerConnection != null,
      'localStream': _localStream != null,
      'remoteStream': _remoteStream != null,
      'localRenderer': _localRenderer != null,
      'remoteRenderer': _remoteRenderer != null,
      'currentRoomId': _currentRoomId,
      'remoteUserId': _remoteUserId,
      'isPeerConnectionReady': _isPeerConnectionReady,
      'queuedIceCandidates': _iceCandidateQueue.length,
    };
  }
  
  /// Check if call is active
  static bool get isCallActive => _peerConnection != null && _localStream != null;
  
  /// Get current room ID
  static String? get currentRoomId => _currentRoomId;
  
  /// Check if remote stream is connected
  static bool get isRemoteStreamConnected => 
      _remoteRenderer != null && _remoteRenderer!.srcObject != null;
  
  /// Get remote stream status
  static String get remoteStreamStatus {
    if (_remoteRenderer == null) return 'Renderer not initialized';
    if (_remoteStream == null) return 'No remote stream';
    if (_remoteRenderer!.srcObject == null) return 'No stream assigned';
    return 'Connected';
  }
  
  /// Force refresh remote stream assignment
  static void refreshRemoteStream() {
    if (_remoteRenderer != null && _remoteStream != null) {
      print('ğŸ”„ Refreshing remote stream assignment...');
      _remoteRenderer!.srcObject = _remoteStream;
      print('ğŸ”Š After refresh: ${_remoteRenderer!.srcObject != null ? "Set" : "Not set"}');
    }
  }
  

  // Private methods for Socket.IO communication

  static void _sendOffer(webrtc.RTCSessionDescription offer, String roomId, String toUserId) {
    print('ğŸ“¤ === SENDING OFFER VIA SOCKET ===');
    print('ğŸ“¤ Room ID: $roomId');
    print('ğŸ“¤ To user: $toUserId');
    print('ğŸ“¤ Offer type: ${offer.type}');
    SocketService.sendWebRTCOffer(
      roomId: roomId,
      toUserId: toUserId,
      offer: offer,
    );
    print('âœ… Offer sent to SocketService');
  }

  static void _sendAnswer(webrtc.RTCSessionDescription answer, String roomId, String toUserId) {
    print('ğŸ“¤ === SENDING ANSWER VIA SOCKET ===');
    print('ğŸ“¤ Room ID: $roomId');
    print('ğŸ“¤ To user: $toUserId');
    print('ğŸ“¤ Answer type: ${answer.type}');
    SocketService.sendWebRTCAnswer(
      roomId: roomId,
      toUserId: toUserId,
      answer: answer,
    );
    print('âœ… Answer sent to SocketService');
  }

  static void _sendIceCandidate(webrtc.RTCIceCandidate candidate) {
    print('ğŸ“¤ === SENDING ICE CANDIDATE VIA SOCKET ===');
    print('ğŸ“¤ Room ID: ${_currentRoomId ?? "NULL"}');
    print('ğŸ“¤ To user: ${_remoteUserId ?? "NULL"}');
    if (_currentRoomId == null || _remoteUserId == null) {
      print('âŒ CRITICAL: Cannot send ICE candidate - roomId or remoteUserId is null!');
      return;
    }
    SocketService.sendWebRTCIceCandidate(
      roomId: _currentRoomId ?? '',
      toUserId: _remoteUserId ?? '',
      candidate: candidate,
    );
    print('âœ… ICE candidate sent to SocketService');
  }

  // Permission dialogs

  static void _showPermissionDeniedDialog() {
    print('ğŸ“± Showing permission denied dialog to user');
    Get.dialog(
      AlertDialog(
        title: const Text('ğŸ¤ Permission Required'),
        content: const Text(
          'This app needs microphone and camera access to make calls.\n\n'
          'Please tap "Settings" and enable:\n'
          'â€¢ Microphone\n'
          'â€¢ Camera (for video calls)\n\n'
          'Then restart the call.',
        ),
        actions: [
          TextButton(
            onPressed: () {
              print('User cancelled permission dialog');
              Get.back();
            },
            child: const Text('Cancel'),
          ),
          TextButton(
            onPressed: () {
              print('User opening app settings to grant permissions');
              Get.back();
              openAppSettings();
            },
            child: const Text('Open Settings'),
          ),
        ],
      ),
    );
  }

  static void _showPermissionPermanentlyDeniedDialog() {
    print('ğŸ“± Showing permanently denied permission dialog to user');
    Get.dialog(
      AlertDialog(
        title: const Text('âš ï¸ Permission Denied'),
        content: const Text(
          'Microphone/Camera permission is permanently denied.\n\n'
          'To enable calls:\n'
          '1. Tap "Open Settings" below\n'
          '2. Go to Permissions\n'
          '3. Enable Microphone and Camera\n'
          '4. Return to the app and try again',
        ),
        actions: [
          TextButton(
            onPressed: () {
              print('User cancelled permanently denied dialog');
              Get.back();
            },
            child: const Text('Cancel'),
          ),
          TextButton(
            onPressed: () {
              print('User opening app settings (permanent denial)');
              Get.back();
              openAppSettings();
            },
            child: const Text('Open Settings'),
          ),
        ],
      ),
    );
  }

  /// Monitor stream health
  static void _monitorStreamHealth() {
    if (_localStream == null) return;
    
    Timer.periodic(const Duration(seconds: 5), (timer) {
      if (_localStream == null) {
        timer.cancel();
        return;
      }
      
      final tracks = _localStream!.getTracks();
      bool hasActiveAudio = false;
      
      for (var track in tracks) {
        if (track.kind == 'audio' && track.enabled) {
          hasActiveAudio = true;
          break;
        }
      }
      
      if (!hasActiveAudio) {
        print('âš ï¸ No active audio tracks detected');
        onCallError?.call('Audio stream lost');
      }
    });
  }

  /// Toggle video track enabled/disabled
  static void toggleVideo(bool enabled) {
    print('========================================');
    print('ğŸ“¹ === TOGGLING VIDEO ===');
    print('ğŸ“¹ New state: ${enabled ? "ENABLED" : "DISABLED"}');
    
    if (_localStream == null) {
      print('âŒ No local stream available to toggle video');
      return;
    }
    
    final videoTracks = _localStream!.getVideoTracks();
    print('ğŸ“¹ Video tracks found: ${videoTracks.length}');
    
    if (videoTracks.isEmpty) {
      print('âš ï¸ No video tracks in local stream');
      return;
    }
    
    for (var track in videoTracks) {
      track.enabled = enabled;
      print('ğŸ“¹ Video track ${track.id} enabled: ${track.enabled}');
    }
    
    print('âœ… Video ${enabled ? "enabled" : "disabled"}');
    print('========================================');
  }
  
  /// Toggle audio track enabled/disabled (mute/unmute)
  static void toggleAudio(bool enabled) {
    print('========================================');
    print('ğŸ¤ === TOGGLING AUDIO ===');
    print('ğŸ¤ New state: ${enabled ? "ENABLED (Unmuted)" : "DISABLED (Muted)"}');
    
    if (_localStream == null) {
      print('âŒ No local stream available to toggle audio');
      return;
    }
    
    final audioTracks = _localStream!.getAudioTracks();
    print('ğŸ¤ Audio tracks found: ${audioTracks.length}');
    
    if (audioTracks.isEmpty) {
      print('âš ï¸ No audio tracks in local stream');
      return;
    }
    
    for (var track in audioTracks) {
      track.enabled = enabled;
      print('ğŸ¤ Audio track ${track.id} enabled: ${track.enabled}');
    }
    
    print('âœ… Audio ${enabled ? "enabled (unmuted)" : "disabled (muted)"}');
    print('========================================');
  }

  /// Dispose service
  static Future<void> dispose() async {
    await endCall();
  }
}